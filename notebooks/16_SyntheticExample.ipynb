{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic \"True Source Probabilities\" Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from pyquantification.experiments import (\n",
    "    cached_experiments,\n",
    "    classification_cache_key,\n",
    "    load_from_cache,\n",
    "    split_test,\n",
    "    DATASETS,\n",
    ")\n",
    "from pyquantification.evaluation import BASE_LAYOUT, t_test, corrected_resampled_t_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_synthetic_dataset(dataset):\n",
    "    # Plot an equal sized source/target set\n",
    "    plot_df = dataset.df[:2000].copy()\n",
    "    plot_df['colour'] = dataset.df['dist'] + '+' + dataset.df['class']\n",
    "\n",
    "    fig = px.histogram(\n",
    "        plot_df,\n",
    "        x='x',\n",
    "        color='colour',\n",
    "    )\n",
    "    fig.update_layout(barmode='overlay')\n",
    "    fig.update_traces(opacity=0.75)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_synthetic_dataset(DATASETS['synthetic-true-prob-no-shift']())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_synthetic_dataset(DATASETS['synthetic-true-prob-prior-shift']())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSLS Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_synthetic_dataset(DATASETS['synthetic-true-prob-gsls-shift']())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "        'dataset_name': 'synthetic-true-prob-no-shift',\n",
    "        'quantifier': 'pcc',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': [\n",
    "            'pcc-pt',\n",
    "            'pcc-apt',\n",
    "            'pcc-mip',\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'dataset_name': 'synthetic-true-prob-prior-shift',\n",
    "        'quantifier': 'em',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': [\n",
    "            'em-pt',\n",
    "            'em-apt',\n",
    "            'em-mip',\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'dataset_name': 'synthetic-true-prob-gsls-shift',\n",
    "        'quantifier': 'gsls',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': [\n",
    "            'ugsls-pt',\n",
    "            'ugsls-apt',\n",
    "            'ugsls-mip',\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "dataset_labels = {\n",
    "    'synthetic-true-prob-no-shift': 'SNS',\n",
    "    'synthetic-true-prob-prior-shift': 'SPS',\n",
    "    'synthetic-true-prob-gsls-shift': 'SGS',\n",
    "}\n",
    "\n",
    "results_dfs = []\n",
    "for config in configs:\n",
    "    print(f'Running {config[\"dataset_name\"]} experiments')\n",
    "    results_dfs.append(cached_experiments(\n",
    "        cache_key=f'sample_{config[\"dataset_name\"]}_rejection_results',\n",
    "        dataset_names=[config['dataset_name']],\n",
    "        classifier_names=['logreg', 'source-prob'],\n",
    "        calibration_methods=['uncalibrated'],\n",
    "        loss_weights=[0],\n",
    "        gain_weights=[0],\n",
    "        random_states=[0],\n",
    "        shift_types=['no_shift'],\n",
    "        bin_counts=['auto'],\n",
    "        random_priors_options=[False],\n",
    "        quantification_methods=[config['quantifier']],\n",
    "        rejectors=config['rejectors'],\n",
    "        rejection_limits=[config['rejection_limit']],\n",
    "        classification_workers=1,\n",
    "        quantification_workers=10,\n",
    "        continue_on_failure=True,\n",
    "        # Run on all samples\n",
    "        sample_idxs=None,\n",
    "    ))\n",
    "results_df = pd.concat(results_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = results_df.copy()\n",
    "plot_df['dataset_label'] = plot_df['dataset_name'].map(dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_coverage_table_latex(table_df):\n",
    "    for _, row in table_df.iterrows():\n",
    "        if row.isna().all():\n",
    "            print('\\hline')\n",
    "        else:\n",
    "            print(' & '.join(row.to_dict().values()).replace('%', '\\%') + r' \\\\')\n",
    "\n",
    "def coverage_table():\n",
    "    experiment_grouping = ['classifier_name', 'dataset_name', 'sample_idx']\n",
    "    plot_methods = {\n",
    "        'pcc': 'PCC',\n",
    "        'em': 'EM',\n",
    "        'gsls': 'GSLS',\n",
    "    }\n",
    "\n",
    "    def format_cell(mean):\n",
    "        str_mean = f'{mean:.0%}'\n",
    "        return str_mean\n",
    "\n",
    "    rows = []\n",
    "    for dataset_name, dataset_label in dataset_labels.items():\n",
    "        for classifier_name in sorted(plot_df['classifier_name'].unique()):\n",
    "            for method, method_label in plot_methods.items():\n",
    "                row = {\n",
    "                    'classifier': classifier_name,\n",
    "                    'dataset': dataset_label,\n",
    "                    'method': method_label,\n",
    "                }\n",
    "                cell_df = plot_df[\n",
    "                    (plot_df['dataset_name'] == dataset_name)\n",
    "                    & (plot_df['classifier_name'] == classifier_name)\n",
    "                ].copy()\n",
    "                # Ensure coverage is numeric so that it is kept during the groupby\n",
    "                cell_df[f'{method}_coverage'] = cell_df[f'{method}_coverage'].astype(float)\n",
    "                # Group by experiment first to group different target_classes together.\n",
    "                cell_df = cell_df.groupby(experiment_grouping, dropna=False).mean().reset_index()\n",
    "                if not pd.isna(cell_df[f'{method}_coverage'].mean()):\n",
    "                    row['coverage'] = format_cell(cell_df[f'{method}_coverage'].mean())\n",
    "                    rows.append(row)\n",
    "        rows.append({})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "coverage_table_df = coverage_table()\n",
    "display(coverage_table_df)\n",
    "print_coverage_table_latex(coverage_table_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rejection_table(table_df):\n",
    "    sdf = table_df.style\n",
    "    sdf.set_table_styles([\n",
    "        {'selector': 'td', 'props': [('text-align', 'left')]},\n",
    "        {'selector': 'th', 'props': [('text-align', 'left')]},\n",
    "    ], overwrite=False)\n",
    "    sdf.set_table_styles({\n",
    "        (classifier, table_df.index.get_level_values(1).unique()[0]): [\n",
    "            {'selector': 'td', 'props': [('border-top', '1px solid black')]},\n",
    "            {'selector': 'th', 'props': [('border-top', '1px solid black')]},\n",
    "        ]\n",
    "        for classifier in table_df.index.get_level_values(0).unique()\n",
    "    }, overwrite=False, axis=1)\n",
    "    sdf.set_table_styles({\n",
    "        (config_label, table_df.columns.get_level_values(1).unique()[0]): [\n",
    "            {'selector': 'td', 'props': [('border-left', '1px solid black')]},\n",
    "            {'selector': 'th', 'props': [('border-left', '1px solid black')]},\n",
    "        ]\n",
    "        for config_label in table_df.columns.get_level_values(0).unique()\n",
    "    }, overwrite=False, axis=0)\n",
    "    display(sdf)\n",
    "\n",
    "\n",
    "def print_rejection_table_latex(table_df):\n",
    "    for keys, row in table_df.iterrows():\n",
    "        if row.isna().all():\n",
    "            print('\\hline')\n",
    "        else:\n",
    "            print(\n",
    "                (' & '.join([\n",
    "                    *keys,\n",
    "                    *row.to_dict().values(),\n",
    "                ]) + r' \\\\')\n",
    "                .replace('%', r'\\%')\n",
    "                .replace('<strong>', r'\\textbf{')\n",
    "                .replace('</strong>', '}')\n",
    "            )\n",
    "    \n",
    "\n",
    "def rejection_table(plot_df, *, dataset_labels, configs, experiment_state_col, baseline_rejector_label,\n",
    "                    t_test_alpha=0.05, corrected_t_test=True, stats=None):\n",
    "    # Pre-compute statistic columns\n",
    "    for config in configs.values():\n",
    "        qua_prefix = config['quantifier']\n",
    "\n",
    "        for rejector in config['rejectors'].values():\n",
    "            rej_prefix = f'{rejector}_{config[\"rejection_limit\"]}'\n",
    "            \n",
    "            plot_df[f'{rej_prefix}_runtime_seconds'] = plot_df[f'{rej_prefix}_all_class_time_ns'] / 1_000_000_000\n",
    "            plot_df[f'{rej_prefix}_rejected_proportion'] = (\n",
    "                plot_df[f'{rej_prefix}_rejected_count'] / plot_df[f'test_n']\n",
    "            )\n",
    "            plot_df[f'{rej_prefix}_coverage_difference'] = (\n",
    "                plot_df[f'{rej_prefix}_coverage'].astype(float) - plot_df[f'{qua_prefix}_coverage'].astype(float)\n",
    "            ).astype(float)\n",
    "            plot_df[f'{rej_prefix}_width_target_diff'] = (\n",
    "                (plot_df[f'{rej_prefix}_target_width_limit'] - plot_df[f'{rej_prefix}_interval_width'])\n",
    "                / plot_df['test_n']\n",
    "            ).astype(float)\n",
    "            plot_df[f'{rej_prefix}_distance_outside_interval'] = (\n",
    "                np.maximum(\n",
    "                    np.maximum(0, plot_df[f'{rej_prefix}_count_lower'] - plot_df['test_true_count']),\n",
    "                    np.maximum(0, plot_df['test_true_count'] - plot_df[f'{rej_prefix}_count_upper']),\n",
    "                ) / plot_df['test_n']\n",
    "            )\n",
    "\n",
    "    table_rows = []\n",
    "    for classifier_name in sorted(plot_df['classifier_name'].unique()):\n",
    "        classifier_rows = {}\n",
    "        for config_label, config in configs.items():\n",
    "            subset_df = plot_df[\n",
    "                (plot_df['classifier_name'] == classifier_name) &\n",
    "                (plot_df['dataset_name'] == config['dataset_name']) &\n",
    "                (~plot_df[f'{config[\"quantifier\"]}_count'].isna())\n",
    "            ]\n",
    "            \n",
    "            def get_stat(col, *, fmt='{:.2f}', std=False, t_test_col=None,\n",
    "                         class_agg='mean', median=False):\n",
    "                selected_cols = [experiment_state_col, col]\n",
    "                if t_test_col is not None:\n",
    "                    selected_cols.append(t_test_col)\n",
    "                \n",
    "                class_groupby = subset_df[set(selected_cols)].groupby(experiment_state_col)\n",
    "                if class_agg == 'mean':\n",
    "                    class_agg_df = class_groupby.mean()\n",
    "                elif class_agg == 'max':\n",
    "                    class_agg_df = class_groupby.max()\n",
    "                elif class_agg == 'min':\n",
    "                    class_agg_df = class_groupby.min()\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError(f'Unrecognised class_agg: {class_agg}')\n",
    "                # Ensure we are only grouping class-rows together in class_max_df\n",
    "                assert (class_agg_df.shape[0] * subset_df['target_class'].nunique()) == subset_df.shape[0]\n",
    "                \n",
    "                if median:\n",
    "                    stat_val = class_agg_df[col].median()\n",
    "                else:\n",
    "                    stat_val = class_agg_df[col].mean()\n",
    "                stat = f'{fmt.format(stat_val)}'\n",
    "                if std:\n",
    "                    stat += f' ({fmt.format(class_agg_df[col].std())})'\n",
    "                if t_test_col is not None and t_test_col != col:\n",
    "                    if corrected_t_test:\n",
    "                        train_n = subset_df['full_train_n'].mean()\n",
    "                        assert np.all(subset_df['full_train_n'] == train_n)\n",
    "                        test_n = subset_df['test_n'].mean()\n",
    "                        assert np.all(subset_df['test_n'] == test_n)\n",
    "                        test_size = test_n / (train_n + test_n)\n",
    "                        p_value = corrected_resampled_t_test(\n",
    "                            class_agg_df[col].to_numpy(),\n",
    "                            class_agg_df[t_test_col].to_numpy(),\n",
    "                            test_size=test_size,\n",
    "                        )\n",
    "                    else:\n",
    "                        p_value = t_test(\n",
    "                            class_agg_df[col].to_numpy(),\n",
    "                            class_agg_df[t_test_col].to_numpy(),\n",
    "                        )\n",
    "                    significant = p_value <= t_test_alpha\n",
    "                    if significant:\n",
    "                        stat = fr'<strong>{stat}</strong>'\n",
    "                return stat\n",
    "\n",
    "            baseline_rej_prefix = f'{config[\"rejectors\"][baseline_rejector_label]}_{config[\"rejection_limit\"]}'\n",
    "            for rejector_label, rejector in config['rejectors'].items():\n",
    "                qua_prefix = config['quantifier']\n",
    "                rej_prefix = f'{rejector}_{config[\"rejection_limit\"]}'\n",
    "                \n",
    "                # Initialise classifier row for this rejector\n",
    "                row = classifier_rows.get(rejector_label, {\n",
    "                    'Classifier': classifier_name,\n",
    "                    'Rejector': rejector_label,\n",
    "                })\n",
    "                # Populate row with stats for this config\n",
    "                row[(config_label, 'Rejection')] = get_stat(\n",
    "                    f'{rej_prefix}_rejected_proportion',\n",
    "                    fmt='{:.1%}',\n",
    "                    std=True,\n",
    "                    t_test_col=f'{baseline_rej_prefix}_rejected_proportion',\n",
    "                )\n",
    "                row[(config_label, 'Interval Width: Limit - Actual')] = get_stat(\n",
    "                    f'{rej_prefix}_width_target_diff',\n",
    "                    fmt='{:.1%}',\n",
    "                    std=True,\n",
    "                    t_test_col=f'{baseline_rej_prefix}_width_target_diff',\n",
    "                    class_agg='min',\n",
    "                )\n",
    "                row[(config_label, 'Coverage: Post - Pre')] = get_stat(\n",
    "                    f'{rej_prefix}_coverage_difference',\n",
    "                    fmt='{:.1%}',\n",
    "                )\n",
    "                row[(config_label, 'Distance From Post-Interval')] = get_stat(\n",
    "                    f'{rej_prefix}_distance_outside_interval',\n",
    "                    fmt='{:.1%}',\n",
    "                    t_test_col=f'{baseline_rej_prefix}_distance_outside_interval',\n",
    "                    std=True,\n",
    "                )\n",
    "                row[(config_label, 'Runtime Seconds')] = get_stat(\n",
    "                    f'{rej_prefix}_runtime_seconds',\n",
    "                    fmt='{:,.2f}',\n",
    "                    std=True,\n",
    "                    t_test_col=f'{baseline_rej_prefix}_runtime_seconds',\n",
    "                    median=True,\n",
    "                )\n",
    "                classifier_rows[rejector_label] = row\n",
    "        table_rows += list(classifier_rows.values())\n",
    "\n",
    "    # Row index: classifier, rejector_label\n",
    "    table_df = pd.DataFrame(table_rows).set_index(['Classifier', 'Rejector'])\n",
    "    # Column index: config_label, stat\n",
    "    table_df.columns = pd.MultiIndex.from_tuples(table_df.columns)\n",
    "    \n",
    "    # Select which stat columns to include\n",
    "    if stats is not None:\n",
    "        table_df = table_df.loc[:, pd.IndexSlice[table_df.columns.get_level_values(0).unique(), stats]]\n",
    "    \n",
    "    return table_df\n",
    "\n",
    "rejection_table_configs = {\n",
    "    'PCC': {\n",
    "        'dataset_name': 'synthetic-true-prob-no-shift',\n",
    "        'quantifier': 'pcc',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'pcc-mip',\n",
    "            'APT': 'pcc-apt',\n",
    "            'PT': 'pcc-pt',\n",
    "        },\n",
    "    },\n",
    "    'EM': {\n",
    "        'dataset_name': 'synthetic-true-prob-prior-shift',\n",
    "        'quantifier': 'em',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'em-mip',\n",
    "            'APT': 'em-apt',\n",
    "            'PT': 'em-pt',\n",
    "        },\n",
    "    },\n",
    "    'GSLS': {\n",
    "        'dataset_name': 'synthetic-true-prob-gsls-shift',\n",
    "        'quantifier': 'gsls',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'ugsls-mip',\n",
    "            'APT': 'ugsls-apt',\n",
    "            'PT': 'ugsls-pt',\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "rejection_table_df = rejection_table(\n",
    "    plot_df,\n",
    "    dataset_labels=dataset_labels,\n",
    "    configs=rejection_table_configs,\n",
    "    experiment_state_col='sample_idx',\n",
    "    baseline_rejector_label='MIP',\n",
    "    corrected_t_test=False,\n",
    "    stats=[\n",
    "        'Rejection',\n",
    "        'Interval Width: Limit - Actual',\n",
    "        'Coverage: Post - Pre',\n",
    "        'Distance From Post-Interval',\n",
    "    ],\n",
    ")\n",
    "display_rejection_table(rejection_table_df)\n",
    "print_rejection_table_latex(rejection_table_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
