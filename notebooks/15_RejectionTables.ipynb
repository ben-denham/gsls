{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limiting Quantification Interval Widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyquantification.experiments import cached_experiments\n",
    "from pyquantification.evaluation import t_test, corrected_resampled_t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = pd.concat([\n",
    "    cached_experiments(cache_key='synthetic_1_rejection_results'),\n",
    "    cached_experiments(cache_key='synthetic_2_rejection_results'),\n",
    "    cached_experiments(cache_key='synthetic_3_rejection_results'),\n",
    "]).reset_index()\n",
    "\n",
    "sample_df = pd.concat([\n",
    "    cached_experiments(cache_key='sample_plankton_1_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_plankton_2_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_plankton_3_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_fg-plankton_1_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_fg-plankton_2_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_fg-plankton_3_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_binary-plankton_1_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_binary-plankton_2_rejection_results'),\n",
    "    cached_experiments(cache_key='sample_binary-plankton_3_rejection_results'),\n",
    "]).reset_index()\n",
    "\n",
    "# Separate runtime experiments that are performed serially.\n",
    "runtime_sample_df = pd.concat([\n",
    "    cached_experiments(cache_key='runtime_sample_plankton_1_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_plankton_2_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_plankton_3_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_fg-plankton_1_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_fg-plankton_2_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_fg-plankton_3_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_binary-plankton_1_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_binary-plankton_2_rejection_results'),\n",
    "    cached_experiments(cache_key='runtime_sample_binary-plankton_3_rejection_results'),\n",
    "]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table_latex(table_df):\n",
    "    for keys, row in table_df.iterrows():\n",
    "        if row.isna().all():\n",
    "            print('\\hline')\n",
    "        else:\n",
    "            print(\n",
    "                (' & '.join([\n",
    "                    *keys,\n",
    "                    *row.to_dict().values(),\n",
    "                ]) + r' \\\\')\n",
    "                .replace('%', r'\\%')\n",
    "                .replace('<strong>', r'\\textbf{')\n",
    "                .replace('</strong>', '}')\n",
    "            )\n",
    "\n",
    "\n",
    "def display_table(table_df):\n",
    "    sdf = table_df.style\n",
    "    sdf.set_table_styles([\n",
    "        {'selector': 'td', 'props': [('text-align', 'left')]},\n",
    "        {'selector': 'th', 'props': [('text-align', 'left')]},\n",
    "    ], overwrite=False)\n",
    "    sdf.set_table_styles({\n",
    "        (dataset, table_df.index.get_level_values(1).unique()[0]): [\n",
    "            {'selector': 'td', 'props': [('border-top', '1px solid black')]},\n",
    "            {'selector': 'th', 'props': [('border-top', '1px solid black')]},\n",
    "        ]\n",
    "        for dataset in table_df.index.get_level_values(0).unique()\n",
    "    }, overwrite=False, axis=1)\n",
    "    sdf.set_table_styles({\n",
    "        (config_label, table_df.columns.get_level_values(1).unique()[0]): [\n",
    "            {'selector': 'td', 'props': [('border-left', '1px solid black')]},\n",
    "            {'selector': 'th', 'props': [('border-left', '1px solid black')]},\n",
    "        ]\n",
    "        for config_label in table_df.columns.get_level_values(0).unique()\n",
    "    }, overwrite=False, axis=0)\n",
    "    display(sdf)\n",
    "\n",
    "\n",
    "def rejection_table(plot_df, *, dataset_labels, configs, experiment_state_col, baseline_rejector_label,\n",
    "                    t_test_alpha=0.05, corrected_t_test=True, stats=None):\n",
    "    # Pre-compute statistic columns\n",
    "    for config in configs.values():\n",
    "        qua_prefix = config['quantifier']\n",
    "\n",
    "        for rejector in config['rejectors'].values():\n",
    "            rej_prefix = f'{rejector}_{config[\"rejection_limit\"]}'\n",
    "            \n",
    "            plot_df[f'{rej_prefix}_runtime_seconds'] = plot_df[f'{rej_prefix}_all_class_time_ns'] / 1_000_000_000\n",
    "            plot_df[f'{rej_prefix}_rejected_proportion'] = (\n",
    "                plot_df[f'{rej_prefix}_rejected_count'] / plot_df[f'test_n']\n",
    "            )\n",
    "            plot_df[f'{rej_prefix}_coverage_difference'] = (\n",
    "                plot_df[f'{rej_prefix}_coverage'] - plot_df[f'{qua_prefix}_coverage']\n",
    "            ).astype(float)\n",
    "            plot_df[f'{rej_prefix}_width_target_diff'] = (\n",
    "                (plot_df[f'{rej_prefix}_target_width_limit'] - plot_df[f'{rej_prefix}_interval_width'])\n",
    "                / plot_df['test_n']\n",
    "            ).astype(float)\n",
    "            plot_df[f'{rej_prefix}_distance_outside_interval'] = (\n",
    "                np.maximum(\n",
    "                    np.maximum(0, plot_df[f'{rej_prefix}_count_lower'] - plot_df['test_true_count']),\n",
    "                    np.maximum(0, plot_df['test_true_count'] - plot_df[f'{rej_prefix}_count_upper']),\n",
    "                ) / plot_df['test_n']\n",
    "            )\n",
    "\n",
    "    table_rows = []\n",
    "    for dataset_name, dataset_label in dataset_labels.items():\n",
    "        dataset_rows = {}\n",
    "        for config_label, config in configs.items():\n",
    "            subset_df = plot_df[\n",
    "                (plot_df['dataset_name'] == dataset_name) &\n",
    "                (plot_df['shift_type'] == config['shift_type']) &\n",
    "                (~plot_df[f'{config[\"quantifier\"]}_count'].isna())\n",
    "            ]\n",
    "            \n",
    "            def get_stat(col, *, fmt='{:.2f}', std=False, t_test_col=None,\n",
    "                         class_agg='mean', median=False):\n",
    "                selected_cols = [experiment_state_col, col]\n",
    "                if t_test_col is not None:\n",
    "                    selected_cols.append(t_test_col)\n",
    "                \n",
    "                class_groupby = subset_df[set(selected_cols)].groupby(experiment_state_col)\n",
    "                if class_agg == 'mean':\n",
    "                    class_agg_df = class_groupby.mean()\n",
    "                elif class_agg == 'max':\n",
    "                    class_agg_df = class_groupby.max()\n",
    "                elif class_agg == 'min':\n",
    "                    class_agg_df = class_groupby.min()\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError(f'Unrecognised class_agg: {class_agg}')\n",
    "                # Ensure we are only grouping class-rows together in class_max_df\n",
    "                assert (class_agg_df.shape[0] * subset_df['target_class'].nunique()) == subset_df.shape[0]\n",
    "                \n",
    "                if median:\n",
    "                    stat_val = class_agg_df[col].median()\n",
    "                else:\n",
    "                    stat_val = class_agg_df[col].mean()\n",
    "                stat = f'{fmt.format(stat_val)}'\n",
    "                if std:\n",
    "                    stat += f' ({fmt.format(class_agg_df[col].std())})'\n",
    "                if t_test_col is not None and t_test_col != col:\n",
    "                    if corrected_t_test:\n",
    "                        train_n = subset_df['full_train_n'].mean()\n",
    "                        assert np.all(subset_df['full_train_n'] == train_n)\n",
    "                        test_n = subset_df['test_n'].mean()\n",
    "                        assert np.all(subset_df['test_n'] == test_n)\n",
    "                        test_size = test_n / (train_n + test_n)\n",
    "                        p_value = corrected_resampled_t_test(\n",
    "                            class_agg_df[col].to_numpy(),\n",
    "                            class_agg_df[t_test_col].to_numpy(),\n",
    "                            test_size=test_size,\n",
    "                        )\n",
    "                    else:\n",
    "                        p_value = t_test(\n",
    "                            class_agg_df[col].to_numpy(),\n",
    "                            class_agg_df[t_test_col].to_numpy(),\n",
    "                        )\n",
    "                    significant = p_value <= t_test_alpha\n",
    "                    if significant:\n",
    "                        stat = fr'<strong>{stat}</strong>'\n",
    "                return stat\n",
    "\n",
    "            baseline_rej_prefix = f'{config[\"rejectors\"][baseline_rejector_label]}_{config[\"rejection_limit\"]}'\n",
    "            for rejector_label, rejector in config['rejectors'].items():\n",
    "                qua_prefix = config['quantifier']\n",
    "                rej_prefix = f'{rejector}_{config[\"rejection_limit\"]}'\n",
    "                \n",
    "                # Initialise dataset row for this rejector\n",
    "                row = dataset_rows.get(rejector_label, {\n",
    "                    'Dataset': dataset_label,\n",
    "                    'Rejector': rejector_label,\n",
    "                })\n",
    "                # Populate row with stats for this config\n",
    "                row[(config_label, 'Rejection')] = get_stat(\n",
    "                    f'{rej_prefix}_rejected_proportion',\n",
    "                    fmt='{:.1%}',\n",
    "                    std=True,\n",
    "                    t_test_col=f'{baseline_rej_prefix}_rejected_proportion',\n",
    "                )\n",
    "                row[(config_label, 'Interval Width: Limit - Actual')] = get_stat(\n",
    "                    f'{rej_prefix}_width_target_diff',\n",
    "                    fmt='{:.1%}',\n",
    "                    std=True,\n",
    "                    t_test_col=f'{baseline_rej_prefix}_width_target_diff',\n",
    "                    class_agg='min',\n",
    "                )\n",
    "                row[(config_label, 'Coverage: Post - Pre')] = get_stat(\n",
    "                    f'{rej_prefix}_coverage_difference',\n",
    "                    fmt='{:.1%}',\n",
    "                )\n",
    "                row[(config_label, 'Distance From Post-Interval')] = get_stat(\n",
    "                    f'{rej_prefix}_distance_outside_interval',\n",
    "                    fmt='{:.1%}',\n",
    "                    t_test_col=f'{baseline_rej_prefix}_distance_outside_interval',\n",
    "                    std=True,\n",
    "                )\n",
    "                row[(config_label, 'Runtime Seconds')] = get_stat(\n",
    "                    f'{rej_prefix}_runtime_seconds',\n",
    "                    fmt='{:,.2f}',\n",
    "                    std=True,\n",
    "                    t_test_col=f'{baseline_rej_prefix}_runtime_seconds',\n",
    "                    median=True,\n",
    "                )\n",
    "                dataset_rows[rejector_label] = row\n",
    "        table_rows += list(dataset_rows.values())\n",
    "\n",
    "    # Row index: dataset_label, rejector_label\n",
    "    table_df = pd.DataFrame(table_rows).set_index(['Dataset', 'Rejector'])\n",
    "    # Column index: config_label, stat\n",
    "    table_df.columns = pd.MultiIndex.from_tuples(table_df.columns)\n",
    "    \n",
    "    # Select which stat columns to include\n",
    "    if stats is not None:\n",
    "        table_df = table_df.loc[:, pd.IndexSlice[table_df.columns.get_level_values(0).unique(), stats]]\n",
    "    \n",
    "    return table_df\n",
    "\n",
    "\n",
    "def mip_rejection_check(plot_df, *, configs):\n",
    "    results = {}\n",
    "    for config_label, config in configs.items():\n",
    "        mip_rejected_count = plot_df[f'{config[\"rejectors\"][\"MIP\"]}_{config[\"rejection_limit\"]}_rejected_count']\n",
    "        apt_rejected_count = plot_df[f'{config[\"rejectors\"][\"APT\"]}_{config[\"rejection_limit\"]}_rejected_count']\n",
    "        results[config_label] = (mip_rejected_count > apt_rejected_count).sum()\n",
    "    return pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset_labels = {\n",
    "    'handwritten-letters-letter': 'HLL',\n",
    "    'handwritten-letters-author': 'HLA',\n",
    "    'arabic-digits': 'DIG',\n",
    "    'insect-sex': 'ISX',\n",
    "    'insect-species': 'ISP',\n",
    "}\n",
    "synthetic_configs = {\n",
    "    'No shift + PCC': {\n",
    "        'shift_type': 'no_shift',\n",
    "        'quantifier': 'pcc',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'pcc-mip',\n",
    "            'APT': 'pcc-apt',\n",
    "            'PT': 'pcc-pt',\n",
    "        },\n",
    "    },\n",
    "    'Prior shift + EM': {\n",
    "        'shift_type': 'prior_shift',\n",
    "        'quantifier': 'em',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'em-mip',\n",
    "            'APT': 'em-apt',\n",
    "            'PT': 'em-pt',\n",
    "        },\n",
    "    },\n",
    "    'GSLS shift + GSLS': {\n",
    "        'shift_type': 'gsls_shift',\n",
    "        'quantifier': 'gsls',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'ugsls-mip',\n",
    "            'APT': 'ugsls-apt',\n",
    "            'PT': 'ugsls-pt',\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "synthetic_table = rejection_table(\n",
    "    synthetic_df,\n",
    "    dataset_labels=synthetic_dataset_labels,\n",
    "    configs=synthetic_configs,\n",
    "    experiment_state_col='random_state',\n",
    "    baseline_rejector_label='MIP',\n",
    "    stats=[\n",
    "        'Rejection',\n",
    "        'Interval Width: Limit - Actual',\n",
    "        'Coverage: Post - Pre',\n",
    "        'Distance From Post-Interval',\n",
    "    ],\n",
    ")\n",
    "display_table(synthetic_table)\n",
    "print_table_latex(synthetic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset_labels = {\n",
    "    'plankton': 'OPL',\n",
    "    'fg-plankton': 'FPL',\n",
    "    'binary-plankton': 'BPL',\n",
    "}\n",
    "sample_configs = {\n",
    "    'PCC': {\n",
    "        'shift_type': 'no_shift',\n",
    "        'quantifier': 'pcc',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'pcc-mip',\n",
    "            'APT': 'pcc-apt',\n",
    "            'PT': 'pcc-pt',\n",
    "        },\n",
    "    },\n",
    "    'EM': {\n",
    "        'shift_type': 'no_shift',\n",
    "        'quantifier': 'em',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'em-mip',\n",
    "            'APT': 'em-apt',\n",
    "            'PT': 'em-pt',\n",
    "        },\n",
    "    },\n",
    "    'GSLS': {\n",
    "        'shift_type': 'no_shift',\n",
    "        'quantifier': 'gsls',\n",
    "        'rejection_limit': 'fracmax:0.5',\n",
    "        'rejectors': {\n",
    "            'MIP': 'ugsls-mip',\n",
    "            'APT': 'ugsls-apt',\n",
    "            'PT': 'ugsls-pt',\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "sample_table = rejection_table(\n",
    "    sample_df,\n",
    "    dataset_labels=sample_dataset_labels,\n",
    "    configs=sample_configs,\n",
    "    experiment_state_col='sample_idx',\n",
    "    baseline_rejector_label='MIP',\n",
    "    corrected_t_test=False,\n",
    "    stats=[\n",
    "        'Rejection',\n",
    "        'Interval Width: Limit - Actual',\n",
    "        'Coverage: Post - Pre',\n",
    "        'Distance From Post-Interval',\n",
    "    ],\n",
    ")\n",
    "display_table(sample_table)\n",
    "print_table_latex(sample_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_sample_table = rejection_table(\n",
    "    runtime_sample_df,\n",
    "    dataset_labels=sample_dataset_labels,\n",
    "    configs=sample_configs,\n",
    "    experiment_state_col='sample_idx',\n",
    "    baseline_rejector_label='MIP',\n",
    "    corrected_t_test=False,\n",
    "    stats=[\n",
    "        'Runtime Seconds',\n",
    "    ],\n",
    ")\n",
    "display_table(runtime_sample_table)\n",
    "print_table_latex(runtime_sample_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_test_counts = runtime_sample_df.groupby('sample_idx').mean()['test_n']\n",
    "print(f'Runtime test sample sizes have mean: {runtime_test_counts.mean()} and std dev: {runtime_test_counts.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIP Correctness Check\n",
    "\n",
    "Given `MIP` rejection uses the same formulation for interval widths as `APT` but has more freedom to select the best set of instances to reject, `MIP` should always reject less than `APT`. However, the underlying MIP solver has been observed to perform excessive rejection in some cases. We check that these cases remain rare, such that their impact on the results can be considered insignificant.\n",
    "\n",
    "There are over 250 cases for sample shift GSLS because there are 5 samples for the plankton dataset that resort to full rejection because of solver errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Synthetic shift')\n",
    "display(mip_rejection_check(synthetic_df, configs=synthetic_configs))\n",
    "print('Sample shift')\n",
    "display(mip_rejection_check(sample_df, configs=sample_configs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
